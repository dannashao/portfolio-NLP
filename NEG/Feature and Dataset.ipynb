{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b93cec65-4370-43db-9e60-76949254ff6d",
   "metadata": {},
   "source": [
    "# Feature extraction and Dataset building\n",
    "This notebook demonstrates how to convert and extract features from [SEM 2012 shared task](http://www.clips.ua.ac.be/sem2012-st-neg/) corpus (cd-sco), then generate huggingface dataset for future use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d22b25-1f27-43d0-b862-bfe60bdc5eb4",
   "metadata": {},
   "source": [
    "## Reading and preprocessing\n",
    "The data is of the following structure:\n",
    "- Each line of the data is a word or a punctuation of the sentence. Different sentences are separated by empty lines.\n",
    "- The first seven columns consist of basic information about the word: file name, sentence number, token number, the word, its lemma, part of speech, and syntactic phrase tree.\n",
    "- **If the sentence does not contain negation, the eighth column will be *** and this will be the end of the row.**\n",
    "- If the sentence does contain negation, this column will be the negation cue. The ninth column will be the negation scope and the tenth column is the negated event.\n",
    "- If the word does not belong to these negation elements, these columns will be marked with _.\n",
    "- **If the sentence contains two or three negations, these three columns will be repeated, so the row will have 13 or 16 columns respectively (The maximum number of negations in a sentence is four across the dataset, resulting in 19 columns in total).**\n",
    "\n",
    "### The goal is to separate sentence with mutiple negations by repeating. That is, make every rows that have more than 10 columns into 10. \n",
    "\n",
    "For example, if a sentence have 3 negations, element [0:7] will be the basic infomation, [8:10] is the negation infomation of the first negation, [11:13] for the second and [14:16] for the third. **We hope to separate this sentence into three rows: [0:10], [0:7, 11:13], [0:7, 14:16]**\n",
    "\n",
    "Implementation details can be found at `convert_conll.py`. With this script, we can convert the conll file into converted data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0bdaa7bb-14ee-4cd2-9006-73d432feee63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sentences_and_labels(df):\n",
    "    \"\"\"\n",
    "    Gets a list of sentences, spaces per sentence, negation cues and a list of negation scopes. \n",
    "    Each sentence is a list of token dicts, spaces per sentence is a list of booleans,\n",
    "    each negation cue is a list of indexes and each negation scope is a list of booleans.\n",
    "    \"\"\"\n",
    "    sentences,spaces,neg_cues,scope_sentences = [],[],[],[]\n",
    "    sentence,sent_spaces,neg_cue,neg_scope = [],[],[],[]\n",
    "    n = 0\n",
    "    for i, row in df.iterrows():\n",
    "        token_dict = row.to_dict()\n",
    "        # Check for the start of a new sentence (token_id == '0')\n",
    "        if row['token_id'] == 0:\n",
    "            n = 0\n",
    "            sentences.append(sentence)\n",
    "            sentence = [token_dict]\n",
    "            \n",
    "            spaces.append(sent_spaces)\n",
    "            sent_spaces = [False if i == len(df)-1 else df['pos_tag'].iloc[i+1].isalpha()]\n",
    "\n",
    "            scope_sentences.append(neg_scope)\n",
    "            neg_scope = [str(row['negation_scope'] != '_')]\n",
    "                \n",
    "            neg_cues.append(neg_cue)\n",
    "            neg_cue = []\n",
    "        else:\n",
    "            n += 1\n",
    "            sentence.append(token_dict)\n",
    "            neg_scope.append(str(row['negation_scope'] != '_'))\n",
    "            sent_spaces.append(False if i == len(df)-1 else df['pos_tag'].iloc[i+1].isalpha())\n",
    "            \n",
    "        if row['negation_word'] != '_' and row['negation_word'] != '***':\n",
    "            neg_cue.append(n)\n",
    "            \n",
    "    return sentences[1:], spaces[1:], neg_cues[1:], scope_sentences[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f72a5d-00a6-4b60-9d60-21ca9d79a940",
   "metadata": {},
   "source": [
    "## Feature engineering\n",
    "For the traditional feature engineering, there are many avaliable features to extract, including using [spaCy](https://spacy.io/) to extract dependency relationships.\n",
    "\n",
    "`extract_features.py` contains the complete feature extraction implementation. See details in the script.\n",
    "\n",
    "Note that we've utilzed GPU for dependency parsing which significantly increase speed. If your device does not have GPU support, comment out `spacy.require_gpu()` from the script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc304d1-4583-4028-8a05-f7ee9c3f354e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import extract_features\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "56499701-5890-4230-9360-ee0171b06d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_and_extract(filename):\n",
    "    raw_df = pd.read_csv(filename, sep='\\t', names=['document_id', 'sentence_id', 'token_id', 'token', 'lemma', 'pos_tag', 'parsing_tree', 'negation_word', 'negation_scope', 'negation_event'])\n",
    "    sentences, spaces, neg_cues, scope_sentences = extract_sentences_and_labels(raw_df)\n",
    "    \n",
    "    X = []\n",
    "    for sentence, sent_spaces, neg_cue in zip(sentences, spaces, neg_cues):\n",
    "        X.append(extract_features.extract_sentence_features(sentence, sent_spaces, neg_cue, extract_features.dependency_parser))\n",
    "    \n",
    "    return X, scope_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cce21922-4e2c-46e6-af74-ed0cf388a46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = filter_and_extract('converted/converted_train.tsv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b9779f-5edd-41b6-9d5d-9239f93b182d",
   "metadata": {},
   "source": [
    "Let's inspect the first sentence from the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5d9c3fe8-c120-40bb-8ef1-7102d8b670cc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'token': 'Chapter',\n",
       "  'neg_type': '',\n",
       "  'lemma': 'Chapter',\n",
       "  'pos_tag': 'NN',\n",
       "  'is_neg': False,\n",
       "  'same_segment': True,\n",
       "  'common_ancester': '',\n",
       "  'cue_distance': 0,\n",
       "  'dependency_relation': 'compound',\n",
       "  'dependency_distance': 0,\n",
       "  'dependency_path': 0},\n",
       " {'token': '1.',\n",
       "  'neg_type': '',\n",
       "  'lemma': '1.',\n",
       "  'pos_tag': 'CD',\n",
       "  'is_neg': False,\n",
       "  'same_segment': True,\n",
       "  'common_ancester': '',\n",
       "  'cue_distance': 0,\n",
       "  'dependency_relation': 'ROOT',\n",
       "  'dependency_distance': 0,\n",
       "  'dependency_path': 0},\n",
       " {'token': 'Mr.',\n",
       "  'neg_type': '',\n",
       "  'lemma': 'Mr.',\n",
       "  'pos_tag': 'NNP',\n",
       "  'is_neg': False,\n",
       "  'same_segment': True,\n",
       "  'common_ancester': '',\n",
       "  'cue_distance': 0,\n",
       "  'dependency_relation': 'pobj',\n",
       "  'dependency_distance': 0,\n",
       "  'dependency_path': 0},\n",
       " {'token': 'Sherlock',\n",
       "  'neg_type': '',\n",
       "  'lemma': 'Sherlock',\n",
       "  'pos_tag': 'NNP',\n",
       "  'is_neg': False,\n",
       "  'same_segment': True,\n",
       "  'common_ancester': '',\n",
       "  'cue_distance': 0,\n",
       "  'dependency_relation': 'conj',\n",
       "  'dependency_distance': 0,\n",
       "  'dependency_path': 0},\n",
       " {'token': 'Holmes',\n",
       "  'neg_type': '',\n",
       "  'lemma': 'Holmes',\n",
       "  'pos_tag': 'NNP',\n",
       "  'is_neg': False,\n",
       "  'same_segment': True,\n",
       "  'common_ancester': '',\n",
       "  'cue_distance': 0,\n",
       "  'dependency_relation': 'punct',\n",
       "  'dependency_distance': 0,\n",
       "  'dependency_path': 0}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "812ecb08-ac0b-45e3-905e-3c7f8880da92",
   "metadata": {},
   "source": [
    "## Generate Huggingface Dataset for future use\n",
    "In this sectipn, we rearrange and convert the dataset to huggingface dataset (see details [here](https://huggingface.co/docs/datasets/v2.16.1/en/package_reference/main_classes#datasets.Dataset.from_pandas)) type for convenience.\n",
    "\n",
    "Since BERT approaches does not need traditional feature engineerings, we extract only id, negation scopes (gold) and tokens.\n",
    "\n",
    "We follow the Augment method described in [NegBERT (Khandelwal, et al. 2020)](http://www.lrec-conf.org/proceedings/lrec2020/pdf/2020.lrec-1.704.pdf).\n",
    "That is, adding a special token ([NEG]) immediately before the predicate:\n",
    "> This is [NEG] not a sentence.\n",
    "\n",
    "Note that **the special token and the predicate is considered a whole**. That is, the actual sentence is like\n",
    "> 'This' 'is' **'[NEG] not'** 'a' 'sentence' '.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5ef0f91e-d2c8-4874-83dc-953b428af460",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e7e6ce90-fef6-4b24-b446-26554b487e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_augment(Xdict, ydict):\n",
    "    '''\n",
    "    This function rearranges datasets with the Augment method described above\n",
    "    '''\n",
    "    redict = []\n",
    "    for x in range(len(Xdict)):\n",
    "        tokens, labels = [],[]\n",
    "        for i, item in enumerate(Xdict[x]):\n",
    "            if not isinstance(item['token'], str):\n",
    "                continue\n",
    "\n",
    "            if item['is_neg']:\n",
    "                tokens.append(f\"[NEG] {item['token']}\")\n",
    "                labels.append(1 if ydict[x][i] == 'True' else 0)\n",
    "            else:\n",
    "                tokens.append(item['token'])\n",
    "                labels.append(1 if ydict[x][i] == 'True' else 0)\n",
    "\n",
    "        redict.append({'id': x, 'negation_scope_tags':labels, 'tokens':tokens})\n",
    "    return redict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "71b45d40-acec-4c44-b126-390b35b4cb81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_huggingface_dataset(Xdict, ydict):\n",
    "    datadict = data_augment(Xdict, ydict)\n",
    "    ds = datasets.Dataset.from_pandas(pd.DataFrame(data=datadict))\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "05fcc973-c3d2-4b89-ad92-b957d369fd62",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_dev, y_dev = filter_and_extract('converted/converted_fulldev.tsv')\n",
    "X_test, y_test = filter_and_extract('converted/converted_test_circle.tsv')\n",
    "X_test2, y_test2 = filter_and_extract('converted/converted_test_cardboard.tsv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8577a960-ba64-4dbd-af86-936d7483036a",
   "metadata": {},
   "source": [
    "The original corpus obtains two test sets for two groups. We combine these groups to form a full test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9f7656b5-1864-4b5e-8983-c9b730a711d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_full = X_test + X_test2\n",
    "y_test_full = y_test + y_test2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c27e6c8b-6535-44a6-a86a-34d44dcbf18c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_ds = to_huggingface_dataset(X_train, y_train)\n",
    "dev_ds = to_huggingface_dataset(X_dev, y_dev)\n",
    "test_full_ds = to_huggingface_dataset(X_test_full, y_test_full)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d4d04ef-2c6e-48ef-9199-32d4b48718e2",
   "metadata": {},
   "source": [
    "We can inspect a sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "50f12eaf-49cf-4a0a-841d-ab074225017d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 10,\n",
       " 'negation_scope_tags': [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0],\n",
       " 'tokens': ['Holmes',\n",
       "  'was',\n",
       "  'sitting',\n",
       "  'with',\n",
       "  'his',\n",
       "  'back',\n",
       "  'to',\n",
       "  'me',\n",
       "  ',',\n",
       "  'and',\n",
       "  'I',\n",
       "  'had',\n",
       "  'given',\n",
       "  'him',\n",
       "  '[NEG] no',\n",
       "  'sign',\n",
       "  'of',\n",
       "  'my',\n",
       "  'occupation',\n",
       "  '.']}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds[10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a8dc280-1748-4b10-831d-83c9bc400ac2",
   "metadata": {},
   "source": [
    "### It's now possible to save the datasets to the disk or push it to huggingface hub\n",
    "\n",
    "To save to the disk, use `train_ds.save_to_disk('...')`\n",
    "\n",
    "To push to the hub, use `train_ds.push_to_hub(\"<organization>/<dataset_id>\", split=\"train\")` for split. \n",
    "\n",
    "(You need to install huggingface_hub and login with `huggingface-cli login` to push to hub with\n",
    "```\n",
    "from huggingface_hub import notebook_login\n",
    "notebook_login()\n",
    "```\n",
    "\n",
    "### You can find this dataset at https://huggingface.co/datasets/dannashao/sem2012forNegbert, and load it with `load_dataset(\"dannashao/sem2012forNegbert\")`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
